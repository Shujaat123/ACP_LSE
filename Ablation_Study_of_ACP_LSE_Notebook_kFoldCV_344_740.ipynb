{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/ACP_LSE/blob/main/Ablation_Study_of_ACP_LSE_Notebook_kFoldCV_344_740.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeEJa2_TZnec"
      },
      "source": [
        "## **ACP-LSE: Anti-cancer peptides classification using deep latent-space encoding of Composition of k-Spaced Amino Acid Pairs**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-m_byicZkQc"
      },
      "source": [
        "This code provide python implementation of ACP-LSE algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LadaflfshqgF"
      },
      "source": [
        "# Loading Useful packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Si6012nzZktl"
      },
      "outputs": [],
      "source": [
        "## Load useful packages\n",
        "import sys, os, re, gc\n",
        "from scipy.io import savemat\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import optimizers\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "from keras.utils import to_categorical # for keras > 2.0\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
        "from keras import optimizers\n",
        "from keras import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import auc, average_precision_score, precision_recall_curve, roc_curve\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from random import sample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import Normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEwrowDzZ_Ah"
      },
      "source": [
        "# Feature-Extraction\n",
        "\n",
        "The CKSAAP feature encoding calculates the frequency of amino acid pairs separated by any k residues. The CKSAAP encoding scheme reflects the amino acid pair information in small and large range with in the peptides depending upon the value of k(gap). The encoding scheme is utilized from iFeature web server, using the following download link: (https://github.com/Superzchen/iFeature).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyMD3sDDxN74",
        "outputId": "2e5e3fd9-ad39-4ac2-cc19-e961f8fe1a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=ebf1ec3c58d3ea69eaef18705eed417f060f8191d756b93ab0791347d9f6604c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting mrmr_selection\n",
            "  Downloading mrmr_selection-0.2.8-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting category-encoders (from mrmr_selection)\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (3.1.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (4.66.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.4.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (2.1.4)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (1.13.1)\n",
            "Requirement already satisfied: polars>=0.12.5 in /usr/local/lib/python3.10/dist-packages (from mrmr_selection) (0.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->mrmr_selection) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->mrmr_selection) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->mrmr_selection) (2024.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders->mrmr_selection) (0.14.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category-encoders->mrmr_selection) (0.5.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mrmr_selection) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mrmr_selection) (2.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category-encoders->mrmr_selection) (1.16.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category-encoders->mrmr_selection) (24.1)\n",
            "Downloading mrmr_selection-0.2.8-py3-none-any.whl (15 kB)\n",
            "Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category-encoders, mrmr_selection\n",
            "Successfully installed category-encoders-2.6.3 mrmr_selection-0.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "dataset_path='https://raw.githubusercontent.com/haichengyi/ACP-DL/master/acp740.txt'\n",
        "wget.download(dataset_path, 'acp740.txt')\n",
        "\n",
        "dataset_path='https://raw.githubusercontent.com/haichengyi/ACP-DL/master/acp240.txt'\n",
        "wget.download(dataset_path, 'acp240.txt')\n",
        "\n",
        "!pip install mrmr_selection\n",
        "from mrmr import mrmr_classif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p258z8dGR0iQ"
      },
      "outputs": [],
      "source": [
        "def Convert_Seq2CKSAAP(train_seq, gap=8):\n",
        "  cksaapfea = []\n",
        "  seq_label = []\n",
        "  for sseq in train_seq:\n",
        "    temp= CKSAAP([sseq], gap=gap)\n",
        "    cksaapfea.append(temp[1][1:])\n",
        "    seq_label.append(sseq[0])\n",
        "\n",
        "  x = np.array(cksaapfea)\n",
        "  y = np.array(seq_label)\n",
        "  y[y=='ACP']=0\n",
        "  y[y=='non-ACP']=1\n",
        "  y[y=='POS']=0\n",
        "  y[y=='NEG']=1\n",
        "  y = to_categorical(y)\n",
        "  print('num pos:', sum(y[:,0]==1), 'num neg:', sum(y[:,0]==0))\n",
        "  return x,y\n",
        "\n",
        "def minSequenceLength(fastas):\n",
        "    minLen = 10000\n",
        "    for i in fastas:\n",
        "        if minLen > len(i[1]):\n",
        "            minLen = len(i[1])\n",
        "    return minLen\n",
        "\n",
        "def CKSAAP(fastas, gap=5, **kw):\n",
        "    if gap < 0:\n",
        "        print('Error: the gap should be equal or greater than zero' + '\\n\\n')\n",
        "        return 0\n",
        "\n",
        "    if minSequenceLength(fastas) < gap+2:\n",
        "        print('Error: all the sequence length should be larger than the (gap value) + 2 = ' + str(gap+2) + '\\n' + 'Current sequence length ='  + str(minSequenceLength(fastas)) + '\\n\\n')\n",
        "        return 0\n",
        "\n",
        "    AA = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "    encodings = []\n",
        "    aaPairs = []\n",
        "    for aa1 in AA:\n",
        "        for aa2 in AA:\n",
        "            aaPairs.append(aa1 + aa2)\n",
        "    header = ['#']\n",
        "    for g in range(gap+1):\n",
        "        for aa in aaPairs:\n",
        "            header.append(aa + '.gap' + str(g))\n",
        "    encodings.append(header)\n",
        "    for i in fastas:\n",
        "        name, sequence = i[0], i[1]\n",
        "        code = [name]\n",
        "        for g in range(gap+1):\n",
        "            myDict = {}\n",
        "            for pair in aaPairs:\n",
        "                myDict[pair] = 0\n",
        "            sum = 0\n",
        "            for index1 in range(len(sequence)):\n",
        "                index2 = index1 + g + 1\n",
        "                if index1 < len(sequence) and index2 < len(sequence) and sequence[index1] in AA and sequence[index2] in AA:\n",
        "                    myDict[sequence[index1] + sequence[index2]] = myDict[sequence[index1] + sequence[index2]] + 1\n",
        "                    sum = sum + 1\n",
        "            for pair in aaPairs:\n",
        "                code.append(myDict[pair] / sum)\n",
        "        encodings.append(code)\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GlxQRVJ87u3A"
      },
      "outputs": [],
      "source": [
        "def load_seq_data(data_path,label):\n",
        "  dataset = pd.read_csv(data_path,names=None,index_col=0, header=None)\n",
        "  seq = []\n",
        "  sample_count = 0\n",
        "\n",
        "  for row in dataset.iterrows():\n",
        "    if(row[0]!='>'):\n",
        "      sample_count = sample_count +1\n",
        "      array = [label, row[0]]\n",
        "      name, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())\n",
        "      seq.append([name, sequence])\n",
        "\n",
        "  print('# of ' + label + ' samples',sample_count)\n",
        "  return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UjctUUgmgnX7"
      },
      "outputs": [],
      "source": [
        "def prepare_feature_acp240_740(path = r\"acp240.txt\"):\n",
        "    # path = r\"acp740.txt\"\n",
        "    # path = r\"acp240.txt\"\n",
        "    new_list=[]\n",
        "    seq_list=[]\n",
        "    label = []\n",
        "    lis = []\n",
        "    interaction_pair = {}\n",
        "    RNA_seq_dict = {}\n",
        "    protein_seq_dict = {}\n",
        "    protein_index = 0\n",
        "    with open(path, 'r') as fp:\n",
        "        for line in fp:\n",
        "            if line[0] == '>':\n",
        "                values = line[1:].strip().split('|')\n",
        "                label_temp = values[1]\n",
        "                proteinName = values[0]\n",
        "                proteinName_1=proteinName.split(\"_\")\n",
        "                new_list.append(proteinName_1[0])\n",
        "    #             print(new_list)\n",
        "\n",
        "                if label_temp == '1':\n",
        "                    label.append(1)\n",
        "                else:\n",
        "                    label.append(0)\n",
        "            else:\n",
        "                seq = line[:-1]\n",
        "                seq_list.append(seq)\n",
        "        for i, item in enumerate(new_list):\n",
        "            lis.append([item, seq_list[i]])\n",
        "\n",
        "    return lis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-92nZhng3_q",
        "outputId": "3967215a-0fa2-4001-e4a0-3860fce120cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of POS samples 138\n",
            "# of NEG samples 206\n",
            "138 206 344\n",
            "240\n",
            "344\n",
            "740\n"
          ]
        }
      ],
      "source": [
        "# ## Loading and pre-processing of dataset\n",
        "pos_all_seq_path = 'https://raw.githubusercontent.com/Shujaat123/ACP_LSE/main/dataset_acp_JTB_2014/1-s2.0-S0022519313004190-mmc1.txt'\n",
        "neg_all_seq_path = 'https://raw.githubusercontent.com/Shujaat123/ACP_LSE/main/dataset_acp_JTB_2014/1-s2.0-S0022519313004190-mmc2.txt'\n",
        "\n",
        "pos_all_seq = load_seq_data(pos_all_seq_path,'POS')\n",
        "neg_all_seq = load_seq_data(neg_all_seq_path,'NEG')\n",
        "\n",
        "ALL_seq344 = pos_all_seq + neg_all_seq\n",
        "\n",
        "print(len(pos_all_seq), len(neg_all_seq), len(ALL_seq344))\n",
        "\n",
        "ALL_seq240=prepare_feature_acp240_740(path = r\"acp240.txt\")\n",
        "ALL_seq740=prepare_feature_acp240_740(path = r\"acp740.txt\")\n",
        "\n",
        "print(len(ALL_seq240))\n",
        "print(len(ALL_seq344))\n",
        "print(len(ALL_seq740))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNyCMw_QK6Bg"
      },
      "source": [
        "## Loading and pre-processing protein's dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar9mtHsEaAyA"
      },
      "source": [
        "# Designing an Auto-Encoder-based classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ge7uF0idR-Gc"
      },
      "outputs": [],
      "source": [
        "## Designing an Auto-Encoder-Classifier model\n",
        "def LSE_Final_Model(input_shape=3600, LV=5, EncNN=10, lambda1=0.999):\n",
        "    # Encoder Network\n",
        "    enc_input = Input(shape=(input_shape,), name='enc_input')\n",
        "    # enc_l1 = Dense(4*EncNN, activation='relu', name='encoder_layer1')(enc_input)\n",
        "    enc_l1 = Dense(50, activation='relu', name='encoder_layer1')(enc_input)\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\n",
        "    enc_l1 = Dropout(rate = 0.3)(enc_l1)\n",
        "\n",
        "    # enc_l2 = Dense(2*EncNN, activation='relu', name='encoder_layer2')(enc_l1)\n",
        "    enc_l2 = Dense(25, activation='relu', name='encoder_layer2')(enc_l1)\n",
        "    enc_l2 = BatchNormalization()(enc_l2)\n",
        "    enc_l2 = Dropout(rate = 0.3)(enc_l2)\n",
        "\n",
        "    # enc_l3 = Dense(EncNN, activation='relu', name='encoder_layer3')(enc_l2)\n",
        "    enc_l3 = Dense(10, activation='relu', name='encoder_layer3')(enc_l2)\n",
        "    enc_l3 = BatchNormalization()(enc_l3)\n",
        "    enc_l3 = Dropout(rate = 0.3)(enc_l3)\n",
        "\n",
        "    encoder_output = Dense(LV, activation='sigmoid', name='encoder_output')(enc_l3)\n",
        "\n",
        "    # Classifier Network\n",
        "    class_l1 = Dense(10, activation='relu', name='class_layer1')(encoder_output)\n",
        "    class_l2 = Dense(10, activation='relu', name='class_layer3')(class_l1)\n",
        "    class_output = Dense(2, activation='softmax', name='class_output')(class_l2)\n",
        "\n",
        "    # Decoder Network\n",
        "    # dec_l1 = Dense(EncNN, activation='relu', name='decoder_layer1')(encoder_output)\n",
        "    dec_l1 = Dense(10, activation='relu', name='decoder_layer1')(encoder_output)\n",
        "    dec_l1 = BatchNormalization()(dec_l1)\n",
        "    dec_l1 = Dropout(rate = 0.3)(dec_l1)\n",
        "\n",
        "    # dec_l2 = Dense(2*EncNN, activation='relu', name='decoder_layer2')(dec_l1)\n",
        "    dec_l2 = Dense(25, activation='relu', name='decoder_layer2')(dec_l1)\n",
        "    dec_l2 = BatchNormalization()(dec_l2)\n",
        "    dec_l2 = Dropout(rate = 0.3)(dec_l2)\n",
        "\n",
        "    # dec_l3 = Dense(4*EncNN, activation='relu', name='decoder_layer3')(dec_l2)\n",
        "    dec_l3 = Dense(50, activation='relu', name='decoder_layer3')(dec_l2)\n",
        "    dec_l3 = BatchNormalization()(dec_l3)\n",
        "    dec_l3 = Dropout(rate = 0.3)(dec_l3)\n",
        "\n",
        "    decoder_output = Dense(input_shape, activation='sigmoid', name='decoder_output')(dec_l3)\n",
        "\n",
        "    model = Model(inputs=[enc_input], outputs=[class_output, decoder_output])\n",
        "\n",
        "    # Compiling model\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss={'class_output': 'binary_crossentropy', 'decoder_output': 'mean_squared_error'},\n",
        "                  # loss_weights={'class_output': 0.001, 'decoder_output': 0.999},\n",
        "                  loss_weights={'class_output': lambda1, 'decoder_output': (1-lambda1)}, # good for 740 dataset\n",
        "                  metrics={'class_output': 'categorical_accuracy', 'decoder_output': 'mse'})  # Specified metrics for both outputs\n",
        "    # Here I used rmsprops optimizer with default values, two objective functions are optimized\n",
        "    # using  weight factors [1 for classifier and 0.1 for decoder loss]\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83mfGGGKaBdj"
      },
      "source": [
        "## Define performance measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zsirQcmlTK7y"
      },
      "outputs": [],
      "source": [
        "## Define performance measures\n",
        "def yoden_index(y, y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\n",
        "  j = (tp/(tp+fn)) + (tn/(tn+fp)) - 1\n",
        "  return j\n",
        "\n",
        "def pmeasure(y, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\n",
        "    sensitivity = tp / (tp + fn )\n",
        "    specificity = tn / (tn + fp)\n",
        "    f1score = (2 * tp) / (2 * tp + fp + fn)\n",
        "    return ({'Sensitivity': sensitivity, 'Specificity': specificity, 'F1-Score': f1score})\n",
        "\n",
        "def Show_Statistics(msg,Stats):\n",
        "  print(msg.upper())\n",
        "  print(70*'-')\n",
        "  print('Accuracy:',Stats[0])\n",
        "  print('Sensitivity:',Stats[1])\n",
        "  print('Specificity:',Stats[2])\n",
        "  print('F1-Score:',Stats[3])\n",
        "  print('MCC:',Stats[4])\n",
        "  print('Balance Accuracy:',Stats[5])\n",
        "  print('Youden-Index:',Stats[6])\n",
        "  print('AUC:',Stats[7])\n",
        "  print('AUPR:',Stats[8])\n",
        "  print('Reconstruction MSE:',Stats[9])\n",
        "  print(70*'-')\n",
        "\n",
        "def Calculate_Stats(y_actual,y_pred, y_score):\n",
        "  acc = accuracy_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  sen = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['Sensitivity']\n",
        "  spe = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['Specificity']\n",
        "  f1 = pmeasure(y_actual.argmax(axis=1), y_pred.argmax(axis=1))['F1-Score']\n",
        "  mcc = matthews_corrcoef(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  bacc = balanced_accuracy_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  yi = yoden_index(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  #auc = roc_auc_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "\n",
        "  pre, rec, _ = precision_recall_curve(y_actual.argmax(axis=1), y_score, pos_label=1)\n",
        "  fpr, tpr, _ = roc_curve(y_actual.argmax(axis=1), y_score, pos_label=1)\n",
        "  auroc = auc(fpr, tpr)\n",
        "  aupr = auc(rec, pre)\n",
        "\n",
        "  return acc, sen, spe, f1, mcc, bacc, yi, auroc, aupr\n",
        "\n",
        "def label_by_th(y_pred, threshold=0.5):\n",
        "  y_pred_copy = y_pred.copy()\n",
        "  y_pred_copy[y_pred>= threshold] = 1\n",
        "  y_pred_copy[y_pred<threshold] = 0\n",
        "  return y_pred_copy\n",
        "\n",
        "def cutoff_youdens_j(fpr,tpr,thresholds):\n",
        "  j_scores = tpr-fpr\n",
        "  j_ordered = sorted(zip(j_scores,thresholds))\n",
        "  return j_ordered[-1][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooB2PHJbaD16"
      },
      "source": [
        "  ## Perform Monte-Carlos Simulations for [num_Trials]\\# of independent Trials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW4-Y7y-Q28O",
        "outputId": "dd4b4d5e-9051-4f6e-de0a-535ce4d6a853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num pos: 376 num neg: 364\n",
            "[66 10 11 61]\n",
            "CKSAAP-Gap: 4 LV= 3 Trial: 0  \n",
            "Training/ Test Youden-index: 1.0 / 0.7156432748538011  \n",
            "Training/ Test MCC: 1.0 / 0.7159704559959585  \n",
            "Training/ Test AUC: 1.0 / 0.9181286549707602  \n",
            "Training/ Test AUPR: 1.0 / 0.9026513215947921  \n",
            "Training/ Test MSE (dB): 6.088296282176957 / 6.0886409498750895\n",
            "[64 11 11 62]\n",
            "CKSAAP-Gap: 4 LV= 3 Trial: 1  \n",
            "Training/ Test Youden-index: 0.9965635738831615 / 0.702648401826484  \n",
            "Training/ Test MCC: 0.9966261558375049 / 0.702648401826484  \n",
            "Training/ Test AUC: 1.0 / 0.9002739726027398  \n",
            "Training/ Test AUPR: 1.0 / 0.8947094206112691  \n",
            "Training/ Test MSE (dB): 6.090567405978514 / 6.091136221829849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f6a516cdd80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f6a516cdd80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[63 12 12 61]\n",
            "CKSAAP-Gap: 4 LV= 3 Trial: 2  \n",
            "Training/ Test Youden-index: 1.0 / 0.6756164383561645  \n",
            "Training/ Test MCC: 1.0 / 0.6756164383561644  \n",
            "Training/ Test AUC: 1.0 / 0.891689497716895  \n",
            "Training/ Test AUPR: 0.9999999999999999 / 0.8977548244460974  \n",
            "Training/ Test MSE (dB): 6.082393617003518 / 6.083176140014559\n"
          ]
        }
      ],
      "source": [
        "## Perform Monte-Carlos Simulations for [num_Trials]# of independent Trials\n",
        "LVs = range(3,4)\n",
        "num_folds = 5\n",
        "gaps = range(4,5)\n",
        "EncNN = 15\n",
        "Lambdas = [0.999]\n",
        "# Lambdas = [0.01]\n",
        "# Dataset_name = 'ACP344'\n",
        "Dataset_name = 'ACP740'\n",
        "\n",
        "if (Dataset_name=='ACP344'):\n",
        "  kf = StratifiedKFold(n_splits=num_folds,shuffle=False)\n",
        "else:\n",
        "  kf = StratifiedKFold(n_splits=num_folds,shuffle=True, random_state=25)\n",
        "\n",
        "# kf = StratifiedKFold(n_splits=10)\n",
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "for gap_ind in range(0,len(gaps)):\n",
        "    if (Dataset_name=='ACP344'):\n",
        "      print('ALL_seq344')\n",
        "      [DataX, LabelY] = Convert_Seq2CKSAAP(ALL_seq344, gap=gaps[gap_ind])\n",
        "    else:\n",
        "      [DataX, LabelY] = Convert_Seq2CKSAAP(ALL_seq740, gap=gaps[gap_ind])\n",
        "    # DataX = preprocessing.normalize(DataX, norm='l2')\n",
        "    # selected_features = mrmr_classif(X=pd.DataFrame(DataX), y=pd.Series(LabelY[:,0]), K=100)\n",
        "    # DataX = DataX[:,selected_features]\n",
        "    for lamda_ind in range(0,len(Lambdas)):\n",
        "      lambda1 = Lambdas[lamda_ind]\n",
        "      for lv_ind in range(0,len(LVs)):\n",
        "        Stats =[]\n",
        "        loop_ind = -1\n",
        "\n",
        "        for train_index, test_index in kf.split(DataX,LabelY.argmax(axis=1)):\n",
        "          loop_ind = loop_ind + 1\n",
        "          X_train, X_test = DataX[train_index], DataX[test_index]\n",
        "          y_train, y_test = LabelY[train_index], LabelY[test_index]\n",
        "\n",
        "          if (Dataset_name=='ACP344'):\n",
        "            print('No Normalization Required')\n",
        "          else:\n",
        "            transformer = Normalizer().fit(X_train)  # fit does nothing.\n",
        "            X_train=transformer.transform(X_train)\n",
        "            X_test=transformer.transform(X_test)\n",
        "\n",
        "          ### FOR SYNTHETIC OVERSAMPLING  ###\n",
        "          # print(X_train.shape,y_train.shape)\n",
        "          # print(X_test.shape,y_test.shape)\n",
        "          # sm = SMOTE(random_state=(gaps[gap_ind]+LVs[lv_ind]+loop_ind+1))\n",
        "          # X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "          # y_train = to_categorical(y_train)\n",
        "\n",
        "          ### FOR FEATURE SELECTION  ###\n",
        "          # selected_features = mrmr_classif(X=pd.DataFrame(X_train), y=pd.Series(y_train[:,0]), K=700)\n",
        "          # X_train = X_train[:,selected_features]\n",
        "          # X_test = X_test[:,selected_features]\n",
        "\n",
        "          model = LSE_Final_Model(input_shape = X_train.shape[1],LV=LVs[lv_ind], EncNN=EncNN, lambda1=lambda1)\n",
        "          es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=50)\n",
        "          checkpoint = ModelCheckpoint('models\\\\model-best.keras',\n",
        "                                      verbose=0, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "          history = model.fit({'enc_input': X_train},\n",
        "                            {'class_output': y_train, 'decoder_output': X_train},\n",
        "                            validation_data = ({'enc_input': X_test},\n",
        "                            {'class_output': y_test, 'decoder_output': X_test}),\n",
        "                            epochs=1000, batch_size=y_train.shape[0], callbacks=[checkpoint, es], verbose=0)\n",
        "          del model  # deletes the existing model\n",
        "          model = load_model('models\\\\model-best.keras')\n",
        "\n",
        "          y_train_pred, X_train_pred = model.predict(X_train,batch_size=1800, verbose=0)\n",
        "          y_train_score = y_train_pred[:,1]\n",
        "          y_train_pred = to_categorical(y_train_pred.argmax(axis=1))\n",
        "          MSE_X_train_pred = (np.square(X_train_pred - X_train)).mean(axis=1)\n",
        "\n",
        "          y_test_pred, X_test_pred = model.predict(X_test,batch_size=200, verbose=0)\n",
        "          y_test_score = y_test_pred[:,1]\n",
        "\n",
        "          # Optimal Threshold\n",
        "          fpr, tpr, thresholds_AUC = roc_curve(y_test.argmax(axis=1), y_test_score)\n",
        "          precision, recall, thresholds_AUPR = precision_recall_curve(y_test.argmax(axis=1),y_test_score)\n",
        "\n",
        "          ## Optimal Threshold metrics\n",
        "          distance = (1-fpr)**2+(1-tpr)**2\n",
        "          EERs = (1-recall)/(1-precision)\n",
        "          positive = sum(y_test.argmax(axis=1))\n",
        "          negative = y_test.shape[0]-positive\n",
        "          ratio = negative/positive\n",
        "          opt_t_AUC = thresholds_AUC[np.argmin(distance)]\n",
        "          opt_t_AUPR = thresholds_AUPR[np.argmin(np.abs(EERs-ratio))]\n",
        "          opt_yodens_j = cutoff_youdens_j(fpr, tpr, thresholds_AUC)\n",
        "          y_test_pred_th = label_by_th(y_test_score, opt_yodens_j)\n",
        "          y_test_pred = to_categorical(y_test_pred_th)\n",
        "          MSE_X_test_pred = (np.square(X_test_pred - X_test)).mean(axis=1)\n",
        "\n",
        "          print(confusion_matrix(y_test.argmax(axis=1), y_test_pred.argmax(axis=1), labels=[0,1]).ravel())\n",
        "\n",
        "          ## Training Measures\n",
        "          tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi, tr_auc, tr_aupr = Calculate_Stats(y_train,y_train_pred, y_train_score);\n",
        "\n",
        "          ## Test Measures\n",
        "          t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr = Calculate_Stats(y_test,y_test_pred, y_test_score);\n",
        "\n",
        "          Stats.append([tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi, tr_auc, tr_aupr, -10*np.log10(MSE_X_train_pred.mean()),\n",
        "                        t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr,-10*np.log10(MSE_X_test_pred.mean())])\n",
        "          print('CKSAAP-Gap:',gaps[gap_ind], 'LV=',LVs[lv_ind],'Trial:',loop_ind,\n",
        "                ' \\nTraining/ Test Youden-index:', tr_yi,'/',t_yi,\n",
        "                ' \\nTraining/ Test MCC:', tr_mcc,'/',t_mcc,\n",
        "                ' \\nTraining/ Test AUC:', tr_auc,'/',t_auc,\n",
        "                ' \\nTraining/ Test AUPR:', tr_aupr,'/',t_aupr,\n",
        "                ' \\nTraining/ Test MSE (dB):', -10*np.log10(MSE_X_train_pred.mean()), '/', -10*np.log10(MSE_X_test_pred.mean()))\n",
        "\n",
        "        del model  # deletes the existing model\n",
        "        Statistics = np.asarray(Stats)\n",
        "        filename = 'ACP_LSE_STATS_CKSAAP_GAP' + str(gaps[gap_ind]) + '_LV' + str(LVs[lv_ind]) + '_lambda' + str(Lambdas[lamda_ind]) + '.mat'\n",
        "        savemat(filename,{'Statistics':Statistics})\n",
        "        print('SAVING... '+ filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddwzm0NGaFsy"
      },
      "source": [
        "## Show Classification/Reconstruction Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSXF9ZkXXv3Y"
      },
      "outputs": [],
      "source": [
        "## Show Classification/Reconstruction Statistics\n",
        "Show_Statistics('Training Results (MEAN)',Statistics.mean(axis=0)[0:10])\n",
        "Show_Statistics('Test Results (MEAN)',Statistics.mean(axis=0)[10:20])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}